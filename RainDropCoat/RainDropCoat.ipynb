{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raindrop (might be wrong?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raindrop arguments\n",
    "dataset = 'P12'  # Choices: 'P12', 'P19', 'eICU', 'PAM'\n",
    "withmissingratio = False  # Missing ratio ranges (0 to 0.5)\n",
    "splittype = 'gender' # Choices: 'random', 'age', 'gender', which is Split type for P12 and P19\n",
    "\n",
    "gender = \"male\" # Used for making embeddings for male or female (New for soruce/target)\n",
    "\n",
    "reverse = False  # Reverse training groups (female/older)\n",
    "feature_removal_level = 'no_removal' # Choices: 'no_removal', 'set', 'sample', Feature removal level (if splittype=random)\n",
    "predictive_label = 'mortality' # Choices: 'mortality', 'LoS' , Predictive label for P12 (mortality or LoS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used:  P12\n",
      "args.dataset, args.splittype, args.reverse, args.withmissingratio, args.feature_removal_level P12 gender False False no_removal\n",
      "missing ratio list [0]\n",
      "Split id: 1\n",
      "5309 1327 1327 5309 1327 1327\n",
      "- - Run 1 - -\n",
      "Stop epochs: 2, Batches/epoch: 1, Total batches: 2\n",
      "Validation: Epoch 0,  val_loss:0.7093, aupr_val: 24.26, auc_val: 59.63\n",
      "**[S] Epoch 0, aupr_val: 24.2650, auc_val: 59.6330 **\n",
      "Training appending\n",
      "Validation: Epoch 1,  val_loss:0.6986, aupr_val: 27.33, auc_val: 66.83\n",
      "**[S] Epoch 1, aupr_val: 27.3311, auc_val: 66.8276 **\n",
      "Validation appending\n",
      "Split id: 2\n",
      "5309 1327 1327 5309 1327 1327\n",
      "- - Run 1 - -\n",
      "Stop epochs: 2, Batches/epoch: 1, Total batches: 2\n",
      "Validation: Epoch 0,  val_loss:0.6922, aupr_val: 20.16, auc_val: 61.24\n",
      "**[S] Epoch 0, aupr_val: 20.1600, auc_val: 61.2444 **\n",
      "Training appending\n",
      "Validation: Epoch 1,  val_loss:0.6963, aupr_val: 23.27, auc_val: 65.42\n",
      "**[S] Epoch 1, aupr_val: 23.2711, auc_val: 65.4157 **\n",
      "Validation appending\n",
      "Split id: 3\n",
      "5309 1327 1327 5309 1327 1327\n",
      "- - Run 1 - -\n",
      "Stop epochs: 2, Batches/epoch: 1, Total batches: 2\n",
      "Validation: Epoch 0,  val_loss:0.6941, aupr_val: 19.61, auc_val: 56.89\n",
      "**[S] Epoch 0, aupr_val: 19.6065, auc_val: 56.8928 **\n",
      "Training appending\n",
      "Validation: Epoch 1,  val_loss:0.6834, aupr_val: 26.57, auc_val: 64.63\n",
      "**[S] Epoch 1, aupr_val: 26.5656, auc_val: 64.6306 **\n",
      "Validation appending\n",
      "Split id: 4\n",
      "5309 1327 1327 5309 1327 1327\n",
      "- - Run 1 - -\n",
      "Stop epochs: 2, Batches/epoch: 1, Total batches: 2\n",
      "Validation: Epoch 0,  val_loss:0.6908, aupr_val: 22.62, auc_val: 56.51\n",
      "**[S] Epoch 0, aupr_val: 22.6196, auc_val: 56.5087 **\n",
      "Training appending\n",
      "Validation: Epoch 1,  val_loss:0.6985, aupr_val: 30.88, auc_val: 63.73\n",
      "**[S] Epoch 1, aupr_val: 30.8758, auc_val: 63.7294 **\n",
      "Validation appending\n",
      "Split id: 5\n",
      "5309 1327 1327 5309 1327 1327\n",
      "- - Run 1 - -\n",
      "Stop epochs: 2, Batches/epoch: 1, Total batches: 2\n",
      "Validation: Epoch 0,  val_loss:0.6877, aupr_val: 21.03, auc_val: 63.06\n",
      "**[S] Epoch 0, aupr_val: 21.0276, auc_val: 63.0627 **\n",
      "Training appending\n",
      "Validation: Epoch 1,  val_loss:0.6909, aupr_val: 24.73, auc_val: 68.94\n",
      "**[S] Epoch 1, aupr_val: 24.7309, auc_val: 68.9354 **\n",
      "Validation appending\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Raindrop.code.raindrop import raindrop_training\n",
    "\n",
    "# raindrop args\n",
    "args = {\n",
    "    'dataset': dataset,\n",
    "    'withmissingratio': withmissingratio,\n",
    "    'splittype': splittype,\n",
    "    'gender' : gender,\n",
    "    'reverse': reverse,\n",
    "    'feature_removal_level': feature_removal_level,\n",
    "    'predictive_label': predictive_label\n",
    "}\n",
    "\n",
    "r\"\"\" Raindrop embeddings\n",
    "The output of raindrop is 5 splits, where each split is just random data from the whole dataset\n",
    "used to asses performance, since the datasets are already made, we will not touch it for now\n",
    "and as for raincoat input, we can either choose 1 split, or merge 5 splits to make a big embedding \n",
    "\"\"\"\n",
    "embeddings = raindrop_training(args)\n",
    "\n",
    "filename = f\"./embeddings/raindrop_{gender}_{dataset}_embeddings.pt\"\n",
    "torch.save(embeddings, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raincoat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Raincoat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Raindrop, we have to Replace the CNN Encoder in RAINCOAT with it:\n",
    "Currently, RAINCOAT uses a standard 1D CNN (CNN class in raincoat.py) to extract time-domain features. This assumes regularly sampled data.\n",
    "We need to replace this CNN encoder with the core architecture of RAINDROP (Raindrop_v2 class in model_rd.py). This will allow RAINCOAT to handle the irregular sampling and capture the complex inter-sensor dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR DECODER:\n",
    "\n",
    "- Reconstruction Logic: The reconstruction logic needs to be adjusted. Here are a few potential approaches:\n",
    "Interpolation: You could use the timestamps and the generated features to interpolate the reconstructed signal at the desired time points.\n",
    "- Time-Aware Decoder: Design a decoder that explicitly incorporates timestamps into its operations. For example, you might use a recurrent neural network (RNN) to process the features and timestamps sequentially, generating the output at each time step.\n",
    "- Masking: If you have missing values (not all timestamps have observations), you might use masking to handle them during reconstruction.\n",
    "- Loss Function: The current L1 reconstruction loss assumes a regular time series. You might need to adapt the loss function to account for irregularity, such as:\n",
    "Weighted Loss: Give more weight to the observed time points during loss calculation.\n",
    "Specialized Loss: Explore loss functions specifically designed for irregular time series reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name this later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dictionary: dict_keys(['samples', 'labels'])\n",
      "(102, 128, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayham\\AppData\\Local\\Temp\\ipykernel_9084\\1745442675.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(f\"data/WISDM/train_1.pt\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading embeddings for RainCoat\n",
    "train_data = torch.load(f\"data/WISDM/train_1.pt\")\n",
    "\n",
    "print(\"Keys in the dictionary:\", train_data.keys())\n",
    "\n",
    "print(train_data['samples'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raincoat arguments\n",
    "# ========  Experiments Name ================\n",
    "save_dir ='experiments_logs' # Directory containing all experiments\n",
    "experiment_description = 'WISDM' # Name of your experiment (EEG, HAR, HHAR_SA, WISDM)\n",
    "\n",
    "# ========= Select the DATASET ==============\n",
    "data_path = r'./data' #Path containing dataset\n",
    "dataset = 'WISDM' # Dataset of choice: (WISDM - EEG - HAR - HHAR_SA, Boiler)\n",
    "\n",
    "# ========= Experiment settings ===============\n",
    "num_runs = 1 # Number of consecutive run with different seeds\n",
    "device = 'cuda' # cpu or cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'save_dir': save_dir,\n",
    "    'experiment_description': experiment_description,\n",
    "    'data_path': data_path,\n",
    "    'dataset': dataset,\n",
    "    'num_runs': feature_removal_level,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "trainer = cross_domain_trainer(args)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
